
 Agenda
 ------
  1. Kafka - Basics & Architecture
  2. Kafka APIs
  3. Kafka Command Line Tools
  4. Kafka Producer API
  5. Kafka Consumer API
  6. Kafka Streams API
  7. Kafka Connector API (introduction)


  Materials
  ---------
   1. PDF Presentations
   2. Code Modules
   3. Class Notes
   Github: https://github.com/ykanakaraju/kafkajava


  

  Event Streaming
  ----------------

    -> Is technological foundation for processing data in real-time where businesses are more data driven

    -> Event streaming is the practice of capturing data in real-time from event sources such as GPS, IoT, 
       Log-streames etc.

  Event Streaming Use-cases
  -------------------------
     -> Finance: Stock exchanges, banks, insurance
     -> Logistics : Tracking cars, trucks
     -> IoT : Captures and process sensor data. 

  Challenges in Streaming Analytics
  ---------------------------------    
    => Collecting data in real time
    => Processing data in real time
    => Data pipeline complexity / unmanagability
    => Data flow volume mismatch between source and target systems.

     -> Messaging Systems are used to decouple data pipelines

     -> Two types of messaging systems

	-> Point-to-point messaging systems (Queue)
		-> Messages produced by a source (producer application) are intended for a specific 
		   sink (consumer application)
		-> After the message is consumed, it will be deleted from the queue		

	-> Publisher-Subscriber messaging systems
		-> Messages are produced by the publishers (producers) to "topics".
		-> Messages in a topic are not intended for any specific consumer (subscriber).
		-> Subscribers can subscribe to the "topics" and process the data
		-> Messages are retained for a pre-configured period (ex: 7 days)
     

  What is Kafka
  -------------
    -> Kafka is distributed streaming platform that is used for:
	  -> Publish and Subscribe streams of records (by external applications)
	  -> Store streams of records in fault-tolerant ways
	  -> Process streams of data as they occurs. 


  Basics components of Kafka
  --------------------------

   1. Zookeeper
	-> Is a coordination service responsible for managing the 'state' of the cluster
	-> All brokers send heartbeats to zookeeper. 
	-> By default runs on port 2181
	-> Mainly used to notify the producer and consumer of any new brokers in the cluster

   2. Brokers
	-> Are systems/servers responsible for storing published data
	-> Each broker is stateless. So they use zookeeper to maintain state.
	-> Each broker has a unique-id inside a kafka cluster
	-> Each broker may have zero or more partitions per topic

   3. Topic 
	-> Is a feed/category to which records are published
	-> Can be consumed by one or more consumers/subscribers
	-> For every topic, kafka maintains partition-logs (distributed commit logs)

   4. Partitions
	-> A topic is organized as partitions.
	-> Each partition is an "ordered commit log"
   
   5. Partition Offset
	-> Each message in a partition has a unique sequence id called "offset"

   6. Replicas
	-> Backups of a partition.
	-> The "id" of the replica is same as the broker-id
	-> They are used to prevent data-loss
	-> Only one replica acts as a 'leader'
		-> All reads and writes are served only by these 'leader' replicas.
		-> If the broker containing 'leader' goes down, an election process is triggered
		   and one the in-sync replicas (ISR) will be elected as the 'leader' 

   7. Cluster
	-> When kafka has more than one broker coordinated by the same ZK, it is called a 'Cluster'

   8. Producer
	   -> Is an application that produce messages to topic (leader) partitions.
           -> A producer can produce messages to one or more topics

   9. Consumer	
	   -> Is an application that subscribes to one or more topics (or topic partitions) and
	      poll messages from the leader replicas of the partitions.



   Getting started with Kafka
   --------------------------

   1. Installing Kafka
	-> Make sure that you have Java 8 installed (JDK 1.8.x)
		java -version  (returns the java version)
	-> Download Apache Kafka binaries and extract it to a suitable location
		URL: https://kafka.apache.org/downloads
	-> This is a the same binaries download for all Operating systems

   2. Understanding the directories
	
	-> bin 	  : you have all the commands (sh & bat files) to start various kafka services
        -> config : all configuration files are located here
	-> libs   : has all the libraries (that you can add to your java projects to start using them)

   3. Start Zookeeper service

	  cd <kafka-installation-directory>	  
	  bin/zookeeper-server-start.sh config/zookeeper.properties

   4. Start Kafka broker service

	cd <kafka-installation-directory>	  
	  bin/kafka-server-start.sh config/server.properties

   5. Topic Operations

		bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic t1
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic t1
		bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topic1 --partitions 3 --replication-factor 1

   6. Console Producer
		bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic1
		bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic1 --property "parse.key=true" --property "key.separator=:"

   7. Console Consumer
		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic1
		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic3 --property print.key=true --property print.value=true --property key.separator=" | "
		bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic3 --property print.key=true --property print.value=true --property key.separator=" | " --from-beginning

		bin/kafka-console-consumer.sh --topic topic1 --bootstrap-server localhost:9092 --group g1
		bin/kafka-console-consumer.sh --topic topic1 --bootstrap-server localhost:9092 --consumer.config config/consumer.properties

   8. Consumer Groups
		bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list


  Single Node, Multi Broker Kafka Cluster
  --------------------------------------

   Create three copies of the server.properties file:

			  	broker.id	port	log.dirs		zookeeper.connect	
	server.properties	0		9092	/tmp/kafka-logs		localhost:2181
	server1.properties	1		9093	/tmp/kafka-logs-1	localhost:2181	
	server2.properties 	2 		9094 	/tmp/kafka-logs-2	localhost:2181



   Message Distribution Logic
   --------------------------
     	1. If the Record contains a partition-id, then the message will be sent to that partition
	2. If the Record has no partition-id specified:
		2.1 If they message has 'key', then check if there is any custom-partitioner specified:
			if yes, the output of the partitioner will decide the partition
			if no, then the default partitoner is used to decide the partition
		2.2 If they message has no 'key', then the messages are distributed among the partitions in
		    a load-balanced manner.
	

   Getting started with creating Java applications in Eclipse
   -----------------------------------------------------------

   1. Make sure you are running Java 8 (JDK 1.8 or up)

   2. Installing Eclipse (Scala IDE for Eclipse)

          Download eclipse binaries from the following link:
		http://scala-ide.org/download/sdk.html
	  Extract the zip file to a folder
	  Go inside the folder and click on Eclispe icon.

  3. Create a simple Java project in Eclispe

	3.1 File -> New -> Java Project
	3.2 Provide a Project Name
	3.3 Click on finish

  4. Adding Kafka libraries to your Java project

	4.1 Directly adding Kafka JAR files to the Java Project
		-> Create a Java project
		-> Right click on the project -> Build Path -> Configure Build Path
		-> Click on "Add External JARs' button
		-> Browser to 'lib' folder of Kafka installation directory
		-> Add all Jar files
		-> Click on 'Apply and Close' button

	4.2 Creating a Maven application 
		-> Create a Java project
		-> Convert it into a maven project
			-> Right click on the project -> Configure -> Convert to Maven project
			-> Keep all defaults
			-> Click on Finish
		-> Open pam.xml file and add dependencies.

				<dependencies>
					<dependency>
						<groupId>org.apache.kafka</groupId>
						<artifactId>kafka-clients</artifactId>
						<version>2.6.1</version>
					</dependency>
					<dependency>
						<groupId>com.mysql</groupId>
						<artifactId>mysql-connector-j</artifactId>
						<version>8.0.32</version>
					</dependency>	
				  </dependencies>

		-> Maven Central URL: https://mvnrepository.com/


=================================================================================
  Serialization:  The process of converting your object into a byte-array
                  Data is stored inside a partition as serialized data

  Deserialization: Converting byte-stream to Object format 
=================================================================================


 Producer: Any external application that reads the source data (streaming data) and 
           publishes the data into a Kafka topic.

         
  KafkaProducer => Producer object which is responsible for maintaining a link to Kafka topic

  Producer<String, String> producer = new KafkaProducer <String, String>(kafkaProps); 
  Properties:  bootstrap.servers, key.serializer, value.serializer
 
                        
  ProducerRecord => Represents a message / record

  producer.send( ProducerRecord )    => RecordMetaData / Exception

======================================================

  Ways to send messages:
  =====================

    Acknowledgement -> Fire & Forget: Ignore 
                    -> Synchronous:  Make 'send' a blocking call and wait until you receive the RecordMetaData 
                    -> Asynchronous: Attach a callback method to capture the return when it arrives.


    1. Fire and Forget => We do not care about the status of the delivery of the message.
                       => Gives you very high through-put.
                       => Low message delivery guarantee.

            Use-Cases: Click Stream Analysis (1000,000 messages/hour)
                       User Behaviour Analysis

           => producer.send( ... )        => Future< RecordMetaData >


    2. Synchronous => We produce a message and wait for the acknowledge
                   => Gives you least through-put
                   => High message delivery guarantee

            Use-Cases:  Financial Trax, Health-Care
            
           => RecordMetaData rms = producer.send( ... ).get()  => RecordMetaData (blocking call)
         

    3. Asychronous => We do care about the acknowledgement, but we do not block the message
                      until acknowledgement is received. 
                   => We attach a Callback method to the produce.send(PR, CBM) method 
                   => These callback run in separate thread and processes the acknowlegments when they 
                      arrive.          

==============================================

  Synchronous (Wait until you get acknowledgment)

      1. Message 1 ==> Ack 1 => Success/Failure  
      2. Messsge 2 ==> Ack 2 => Success/Failure 

  Asynchronous 

     1. 1:00:01 Message 1 ==> Callback @ 1:00:05 
     2. 1:00:02 Message 2 ==> Callback @ 1:00:03
     3. 1:00:03 Message 3 ==> Callback

     
     1:00:01
     Message 1 failed and kafka retries for 3 times and then it succeeded, so it sends an Ack @ 1:00:05

     1:00:02
     Message 2 succeeded in the first attemp, then ack is sent at 1:00:03

 ==================================================================

  Producer Configurations
  -----------------------

   1. bootstrap.server (required)

   2. key.serializer (required)

   3. value.serializer (required)

   4. buffer.memory
	
       -> The total memory the producer can use to buffer records waiting to be sent to server. 
 
   5. max.block.ms (def: 60000)
	
	-> Controls how lond send() method will block. This method may block because is full.

   6. batch.size (def: 16384 i.e 16KB)

	-> The producer will attempt to batch records into fewer requests whenever records are 
           being sent to same partition.  

    7. compression.type (def: none)
	
	-> The compression type for all the batches generated by the producer.
	-> Values: none, gzip, snappy, lz4, zstd


    8. max.request.size  => maximum size of each record. 

    9. retries  (def: Integer.Max)
	
	-> Causes the client to resend any record whose send fails with a retrieable-error. 

    10. delivery.timeout.ms (def: 120000)

	-> An upperbound on the time to report sucess or failure after the call to send succeed. 

    11. linger.ms

    12. acks (def: 1)  
	
	-> Values: all (or -1), 1, 0
	
	-> The number of acknowledgements the producer requires the leader to have received before
           considering a request complete.

	acks = 0  => The producer will not wait for the acks from the server at all. 

        acks = 1  => The leader will write the record to its local log will respond without awaiting full 
                     acks from the followers.

        acks = all (safe producer)
		=> The leader will wait for all in-sync replicas to acknowledge

    13. partitioner.class



   In-Sync Replicas  
   ----------------
      ISR is simply all the replicas of the partition that are "in-sync" with the leader
      replica.lag.time.max.ms (def: 10000)
           
   
   Partitions
   ----------

    => partitioning of messages is dicided based on the "key" of the message.

    => All the messages with the "same key" will be sent to the same partition, as long as 
       the number of partitions of the topics remains the same. 

      Let us assume that your topic has 3 partitions => 0, 1, 2, 3


   Key         HashCode      Partition-Id
   ---         --------      -------------
   USA          2600     	 2         
   USA          2600             2
   India        2604		 0
   UK           2607		 0
   India        2604		 0 
   UK           2607		 0  
   USA          2600             2 
   Germany      2608	         1
   Germany      2608	         1
  ---------------------------------
   USA          2600     	 0
   USA          2600     	 0
   India        2604		 0
   India        2604		 0
   UK           2607		 3    


  Consumer API
  ------------

   => A consumer is a client application that connects to kafka cluster, subscribes to one
      (or more) topic(s) or to a few partitions of a topic. 
    
       -> A consumer may subscribe to a single topic
          (it consumes the data of all partitions of that topics)

       -> A consumer may subscribe to multiple topics
          (it consumes data from all partitions of all the subscribed topics possibly to create
           enriched streams)

       -> A consumer can subscribe to one or more partitions of a topic
          (usually to process only specific key's data)

   => Kafka consumers have a "poll" model, while other enterprize message-buses have "pull" model.
      
      -> This allows consumers to control from where in the commit-log (partition) they want to
         consume - from the earliest message, from latest message or from a specific off-set.
   
      
    => Problems with a "standalone" consumer

      -> If the consumption throughput is slower than production throughput (which is the common case)
         then there could be two problems:
    
         -> messages may not be processed at all due to lag. (because of retention-period)
         -> message processing can be time-sensitive. Too much lag could be detrimental to the business.
         -> Consumer-groups can solve this problem.


   Basic steps for creating a Consumer client application
   ------------------------------------------------------

   1. Difine the "Properties" object

	-> bootstrap.servers
	-> group.id (optional)
	-> key.deserializer
	-> value.deserializer

   2. Create a KafkaConsumer object passing the 'Properties' as an argument

   3. Subscribe the consumer to a topic (or topic partition)

   4. Create a poll loop (which is an infinite loop)
	-> Call consumer.poll method to poll for the messages from all the partitions


       
    Partition Rebalancing for Consumer Groups
    -----------------------------------------
  
   	=> Moving the ownership from one consumer instance (in a consumer group) to another consumer instance.

   Consumer Offset Commit Strategies
    ---------------------------------

     1. Atmost once
    
          -> offsets are committed 'as soon as' the messages are received by the consumer.

          -> If the processing goes wrong, and rebalencing happens, there is possibility that
             some of the messages are not processed at all.         


     2. Atleast once (default)

           -> Offsets are committed "after" the messages are processed. 
           -> Of the process goes wrong, the messages will be read again.
           -> This can result in duplicate processing of the messages.

           -> Make sure your processing is idempotent (like upserting) so the duplicates
              does not create problems.


     3. Exactly once

            -> Available only in "Kafka to Kafka" workflow. 



   Auto Offset Reset
   -----------------

   -> This tells what should happen when the message corresponding the commited offset 
      is not found. Messages may not found for a consumer if the consumer is down for more
      than 7 day and the messages were purged by Kafka (becase Kafka stores messages for only 
      7 days)

      values: "latest" (default), "earliest", "none"  



  Resetting the current offsets of a consumer group
  -------------------------------------------------
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --describe
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --topic ctstopic  --reset-offsets --to-offset 100 --execute
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --topic ctstopic  --reset-offsets --to-earliest --execute
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --topic ctstopic  --reset-offsets --to-latest --execute
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --topic ctstopic  --reset-offsets --shift-by -10 --execute
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group ctstopic-group-api --topic ctstopic  --reset-offsets --shift-by 5 --execute



  Consumer Configurations
  -----------------------

   1. fetch.min.bytes (def: 1)

	-> Controls how much data to pull in each request

   2. max.poll.records (def: 500)
	
	-> controls how many messages to receive per poll request
	-> increase if messages are small and you have lot of RAM

   3. max.partitions.fetch.bytes (def: 1 MB)
	
	-> Maximum data returned by the broker per partition

   4. fetch.max.bytes (def: 50 MB)
	
	-> maximum data returned for each fetch request (covers multiple partitions)
        -> Consumer performs multiple fetches in parallel. 

  5. heartbeat.intervel.ms (def: 3000)

  6. session.timeout.ms (def: 10000)

  7. allow.auto.create.topics (def: true)

  8. enable.auto.commit (def: true)

  9. auto.offset.reset	(def: latest) => latest, earliest, none
	
	-> Sets which offset the consumer (in the group) should start reciving from. 
        -> Applicable when the consumer of coming for the very first time or if the available offset is invalid.
 
  10. partition.assignment.strategy (roundrobin & range)


    How Consumer Group Works
    ------------------------

    => Whenever you launch the first consumer in a consumer-group, one of the brokers in the
       cluster is designated as the "Group Coordinator". 

    => The first consumer in the group is a called a "leader consumer".       

    => Every consumer in the consumer group maintains a membership in the consumer group by
       sending "heart-beats" to the Group Coordinator. If the coordinator does not recives the
       heart-beats for some amount of time designated by "session timeout", then automatically,
       the coordinator senses that the consumer is dead and triggers a rebalance.

           heartbeat.interval.ms = 3000  (3 sec)
           session.timeout.ms = 10000 (10 sec)
	 
































































